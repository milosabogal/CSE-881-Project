{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda')\n",
    "#     print('GPU')\n",
    "# else:\n",
    "#     device = torch.device('cpu')\n",
    "#     print('CPU')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yfinance_data(ticker_list : list[str], start_date : str, end_date : str):\n",
    "    \"\"\" Get data from yfinance for a list of tickers.\n",
    "    \n",
    "    It includes Open, High, Low, Close, Adj. Close, and Volume.\n",
    "    \n",
    "    Args:\n",
    "        ticker_list: List of ticker symbols\n",
    "        start_date: Start date of the data\n",
    "        end_date: End date of the data\n",
    "\n",
    "    Returns:\n",
    "        A dict mapping ticker symbols to dataframes containing the data\n",
    "        fetched from yfinance\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "\n",
    "    for ticker_symbol in ticker_list:\n",
    "        data = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
    "        data_dict[ticker_symbol] = data\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Example tickers and dates\n",
    "stock1 = \"GOOGL\" #\"AAPL\"\n",
    "stock2 = \"DBX\" #\"MSFT\"\n",
    "stock3 = \"V\" #\"TSLA\"\n",
    "\n",
    "tickers = [stock1, stock2, stock3]\n",
    "\n",
    "start_date = \"2021-01-01\"\n",
    "end_date = \"2024-01-01\"\n",
    "\n",
    "yfinance_data_dict = get_yfinance_data(tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-11-17</th>\n",
       "      <td>136.000000</td>\n",
       "      <td>136.059998</td>\n",
       "      <td>133.649994</td>\n",
       "      <td>135.309998</td>\n",
       "      <td>135.309998</td>\n",
       "      <td>37240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-20</th>\n",
       "      <td>133.690002</td>\n",
       "      <td>136.660004</td>\n",
       "      <td>133.619995</td>\n",
       "      <td>136.250000</td>\n",
       "      <td>136.250000</td>\n",
       "      <td>27815500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-21</th>\n",
       "      <td>136.289993</td>\n",
       "      <td>137.179993</td>\n",
       "      <td>135.960007</td>\n",
       "      <td>136.970001</td>\n",
       "      <td>136.970001</td>\n",
       "      <td>22635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-22</th>\n",
       "      <td>137.470001</td>\n",
       "      <td>139.419998</td>\n",
       "      <td>137.470001</td>\n",
       "      <td>138.490005</td>\n",
       "      <td>138.490005</td>\n",
       "      <td>17813900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-11-24</th>\n",
       "      <td>138.029999</td>\n",
       "      <td>138.130005</td>\n",
       "      <td>135.990005</td>\n",
       "      <td>136.690002</td>\n",
       "      <td>136.690002</td>\n",
       "      <td>12514300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2023-11-17  136.000000  136.059998  133.649994  135.309998  135.309998   \n",
       "2023-11-20  133.690002  136.660004  133.619995  136.250000  136.250000   \n",
       "2023-11-21  136.289993  137.179993  135.960007  136.970001  136.970001   \n",
       "2023-11-22  137.470001  139.419998  137.470001  138.490005  138.490005   \n",
       "2023-11-24  138.029999  138.130005  135.990005  136.690002  136.690002   \n",
       "\n",
       "              Volume  \n",
       "Date                  \n",
       "2023-11-17  37240600  \n",
       "2023-11-20  27815500  \n",
       "2023-11-21  22635300  \n",
       "2023-11-22  17813900  \n",
       "2023-11-24  12514300  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example dataframe\n",
    "example_df = yfinance_data_dict[stock1]\n",
    "example_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 3\n",
    "\n",
    "SMA_PERIOD = WINDOW_SIZE\n",
    "EMA_PERIOD = WINDOW_SIZE\n",
    "CCI_PERIOD = WINDOW_SIZE\n",
    "VOLATILITY_PERIOD  = WINDOW_SIZE\n",
    "ROC_PERIOD  = WINDOW_SIZE\n",
    "\n",
    "for ticker, ticker_df in yfinance_data_dict.items():\n",
    "    ticker_df[f\"Returns {ticker}\"] = (ticker_df[\"Close\"] - ticker_df[\"Open\"]) / ticker_df[\"Open\"]\n",
    "    \n",
    "    ticker_df[f\"Log Returns {ticker}\"] = np.log(ticker_df[\"Close\"]).diff()\n",
    "    \n",
    "    ticker_df[f\"SMA {ticker}\"] = ticker_df[\"Close\"].rolling(window=SMA_PERIOD).mean()\n",
    "    \n",
    "    ticker_df[f\"EMA {ticker}\"] = ticker_df[\"Close\"].ewm(span=EMA_PERIOD, adjust=False).mean()\n",
    "\n",
    "    # VWAP\n",
    "    value = ticker_df[\"Close\"] * ticker_df[\"Volume\"]\n",
    "    cumulative_value = value.cumsum()\n",
    "    cumulative_volume = ticker_df[\"Volume\"].cumsum()\n",
    "    ticker_df[f\"VWAP {ticker}\"] = cumulative_value / cumulative_volume\n",
    "\n",
    "    # CCI \n",
    "    typical_price = (ticker_df[\"High\"] + ticker_df[\"Low\"] + ticker_df[\"Close\"]) / 3\n",
    "    mean_typical_price = typical_price.rolling(window=CCI_PERIOD).mean()\n",
    "    mean_deviation = (typical_price - mean_typical_price).abs().rolling(window=CCI_PERIOD).mean()\n",
    "    ticker_df[f\"CCI {ticker}\"] = (typical_price - mean_typical_price) / (0.015 * mean_deviation)\n",
    "\n",
    "    ticker_df[f\"Volatility {ticker}\"] = ticker_df[f\"Returns {ticker}\"].rolling(window=VOLATILITY_PERIOD).std()\n",
    "\n",
    "    ticker_df[f\"RoC {ticker}\"] = (ticker_df[\"Close\"] / ticker_df[\"Close\"].shift(ROC_PERIOD) - 1) * 100\n",
    "\n",
    "    ticker_df.rename(columns={'Volume' : f\"Volume {ticker}\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker_df in yfinance_data_dict.values():\n",
    "    ticker_df.drop([\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"], axis = 1, inplace=True)\n",
    "    ticker_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume GOOGL</th>\n",
       "      <th>Returns GOOGL</th>\n",
       "      <th>Log Returns GOOGL</th>\n",
       "      <th>SMA GOOGL</th>\n",
       "      <th>EMA GOOGL</th>\n",
       "      <th>VWAP GOOGL</th>\n",
       "      <th>CCI GOOGL</th>\n",
       "      <th>Volatility GOOGL</th>\n",
       "      <th>RoC GOOGL</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-08</th>\n",
       "      <td>35484000</td>\n",
       "      <td>0.011631</td>\n",
       "      <td>0.013152</td>\n",
       "      <td>88.250834</td>\n",
       "      <td>88.724814</td>\n",
       "      <td>87.599331</td>\n",
       "      <td>101.889946</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>3.320594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-11</th>\n",
       "      <td>34796000</td>\n",
       "      <td>-0.011666</td>\n",
       "      <td>-0.023377</td>\n",
       "      <td>88.807668</td>\n",
       "      <td>88.269657</td>\n",
       "      <td>87.633915</td>\n",
       "      <td>-19.974084</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>1.939197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-12</th>\n",
       "      <td>29528000</td>\n",
       "      <td>-0.004543</td>\n",
       "      <td>-0.010797</td>\n",
       "      <td>88.192500</td>\n",
       "      <td>87.570577</td>\n",
       "      <td>87.542406</td>\n",
       "      <td>-77.760797</td>\n",
       "      <td>0.011938</td>\n",
       "      <td>-2.080215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-13</th>\n",
       "      <td>23432000</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.005636</td>\n",
       "      <td>87.349500</td>\n",
       "      <td>87.466540</td>\n",
       "      <td>87.526761</td>\n",
       "      <td>-33.705934</td>\n",
       "      <td>0.011887</td>\n",
       "      <td>-2.813391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-14</th>\n",
       "      <td>29212000</td>\n",
       "      <td>-0.010264</td>\n",
       "      <td>-0.009390</td>\n",
       "      <td>86.926666</td>\n",
       "      <td>87.006269</td>\n",
       "      <td>87.430833</td>\n",
       "      <td>-2.719865</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>-1.444524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Volume GOOGL  Returns GOOGL  Log Returns GOOGL  SMA GOOGL  \\\n",
       "Date                                                                    \n",
       "2021-01-08      35484000       0.011631           0.013152  88.250834   \n",
       "2021-01-11      34796000      -0.011666          -0.023377  88.807668   \n",
       "2021-01-12      29528000      -0.004543          -0.010797  88.192500   \n",
       "2021-01-13      23432000       0.011538           0.005636  87.349500   \n",
       "2021-01-14      29212000      -0.010264          -0.009390  86.926666   \n",
       "\n",
       "            EMA GOOGL  VWAP GOOGL   CCI GOOGL  Volatility GOOGL  RoC GOOGL  \n",
       "Date                                                                        \n",
       "2021-01-08  88.724814   87.599331  101.889946          0.008751   3.320594  \n",
       "2021-01-11  88.269657   87.633915  -19.974084          0.019725   1.939197  \n",
       "2021-01-12  87.570577   87.542406  -77.760797          0.011938  -2.080215  \n",
       "2021-01-13  87.466540   87.526761  -33.705934          0.011887  -2.813391  \n",
       "2021-01-14  87.006269   87.430833   -2.719865          0.011304  -1.444524  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example dataframe after feature construction\n",
    "example_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat(yfinance_data_dict.values(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"Label\"] = np.argmax(full_df[[f\"Returns {ticker}\" for ticker in tickers]].values, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume GOOGL</th>\n",
       "      <th>Returns GOOGL</th>\n",
       "      <th>Log Returns GOOGL</th>\n",
       "      <th>SMA GOOGL</th>\n",
       "      <th>EMA GOOGL</th>\n",
       "      <th>VWAP GOOGL</th>\n",
       "      <th>CCI GOOGL</th>\n",
       "      <th>Volatility GOOGL</th>\n",
       "      <th>RoC GOOGL</th>\n",
       "      <th>Volume DBX</th>\n",
       "      <th>...</th>\n",
       "      <th>Volume V</th>\n",
       "      <th>Returns V</th>\n",
       "      <th>Log Returns V</th>\n",
       "      <th>SMA V</th>\n",
       "      <th>EMA V</th>\n",
       "      <th>VWAP V</th>\n",
       "      <th>CCI V</th>\n",
       "      <th>Volatility V</th>\n",
       "      <th>RoC V</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-08</th>\n",
       "      <td>35484000</td>\n",
       "      <td>0.011631</td>\n",
       "      <td>0.013152</td>\n",
       "      <td>88.250834</td>\n",
       "      <td>88.724814</td>\n",
       "      <td>87.599331</td>\n",
       "      <td>101.889946</td>\n",
       "      <td>0.008751</td>\n",
       "      <td>3.320594</td>\n",
       "      <td>5240100</td>\n",
       "      <td>...</td>\n",
       "      <td>6513000</td>\n",
       "      <td>0.005977</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>213.959997</td>\n",
       "      <td>214.771872</td>\n",
       "      <td>214.948429</td>\n",
       "      <td>45.644519</td>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.438209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-11</th>\n",
       "      <td>34796000</td>\n",
       "      <td>-0.011666</td>\n",
       "      <td>-0.023377</td>\n",
       "      <td>88.807668</td>\n",
       "      <td>88.269657</td>\n",
       "      <td>87.633915</td>\n",
       "      <td>-19.974084</td>\n",
       "      <td>0.019725</td>\n",
       "      <td>1.939197</td>\n",
       "      <td>8838700</td>\n",
       "      <td>...</td>\n",
       "      <td>7353100</td>\n",
       "      <td>-0.005001</td>\n",
       "      <td>-0.011953</td>\n",
       "      <td>214.049998</td>\n",
       "      <td>213.830935</td>\n",
       "      <td>214.640966</td>\n",
       "      <td>-61.141120</td>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.126989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-12</th>\n",
       "      <td>29528000</td>\n",
       "      <td>-0.004543</td>\n",
       "      <td>-0.010797</td>\n",
       "      <td>88.192500</td>\n",
       "      <td>87.570577</td>\n",
       "      <td>87.542406</td>\n",
       "      <td>-77.760797</td>\n",
       "      <td>0.011938</td>\n",
       "      <td>-2.080215</td>\n",
       "      <td>5722900</td>\n",
       "      <td>...</td>\n",
       "      <td>9331600</td>\n",
       "      <td>-0.016389</td>\n",
       "      <td>-0.019111</td>\n",
       "      <td>212.399999</td>\n",
       "      <td>211.345468</td>\n",
       "      <td>213.719760</td>\n",
       "      <td>-138.656181</td>\n",
       "      <td>0.011183</td>\n",
       "      <td>-2.315138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-13</th>\n",
       "      <td>23432000</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.005636</td>\n",
       "      <td>87.349500</td>\n",
       "      <td>87.466540</td>\n",
       "      <td>87.526761</td>\n",
       "      <td>-33.705934</td>\n",
       "      <td>0.011887</td>\n",
       "      <td>-2.813391</td>\n",
       "      <td>14511800</td>\n",
       "      <td>...</td>\n",
       "      <td>6688500</td>\n",
       "      <td>-0.000287</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>210.366669</td>\n",
       "      <td>210.347737</td>\n",
       "      <td>213.271824</td>\n",
       "      <td>-53.236638</td>\n",
       "      <td>0.008278</td>\n",
       "      <td>-2.831279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-14</th>\n",
       "      <td>29212000</td>\n",
       "      <td>-0.010264</td>\n",
       "      <td>-0.009390</td>\n",
       "      <td>86.926666</td>\n",
       "      <td>87.006269</td>\n",
       "      <td>87.430833</td>\n",
       "      <td>-2.719865</td>\n",
       "      <td>0.011304</td>\n",
       "      <td>-1.444524</td>\n",
       "      <td>6976100</td>\n",
       "      <td>...</td>\n",
       "      <td>12887500</td>\n",
       "      <td>-0.041364</td>\n",
       "      <td>-0.036433</td>\n",
       "      <td>206.690002</td>\n",
       "      <td>206.103869</td>\n",
       "      <td>211.389589</td>\n",
       "      <td>-86.102763</td>\n",
       "      <td>0.020698</td>\n",
       "      <td>-5.181079</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Volume GOOGL  Returns GOOGL  Log Returns GOOGL  SMA GOOGL  \\\n",
       "Date                                                                    \n",
       "2021-01-08      35484000       0.011631           0.013152  88.250834   \n",
       "2021-01-11      34796000      -0.011666          -0.023377  88.807668   \n",
       "2021-01-12      29528000      -0.004543          -0.010797  88.192500   \n",
       "2021-01-13      23432000       0.011538           0.005636  87.349500   \n",
       "2021-01-14      29212000      -0.010264          -0.009390  86.926666   \n",
       "\n",
       "            EMA GOOGL  VWAP GOOGL   CCI GOOGL  Volatility GOOGL  RoC GOOGL  \\\n",
       "Date                                                                         \n",
       "2021-01-08  88.724814   87.599331  101.889946          0.008751   3.320594   \n",
       "2021-01-11  88.269657   87.633915  -19.974084          0.019725   1.939197   \n",
       "2021-01-12  87.570577   87.542406  -77.760797          0.011938  -2.080215   \n",
       "2021-01-13  87.466540   87.526761  -33.705934          0.011887  -2.813391   \n",
       "2021-01-14  87.006269   87.430833   -2.719865          0.011304  -1.444524   \n",
       "\n",
       "            Volume DBX  ...  Volume V  Returns V  Log Returns V       SMA V  \\\n",
       "Date                    ...                                                   \n",
       "2021-01-08     5240100  ...   6513000   0.005977       0.007641  213.959997   \n",
       "2021-01-11     8838700  ...   7353100  -0.005001      -0.011953  214.049998   \n",
       "2021-01-12     5722900  ...   9331600  -0.016389      -0.019111  212.399999   \n",
       "2021-01-13    14511800  ...   6688500  -0.000287       0.002343  210.366669   \n",
       "2021-01-14     6976100  ...  12887500  -0.041364      -0.036433  206.690002   \n",
       "\n",
       "                 EMA V      VWAP V       CCI V  Volatility V     RoC V  Label  \n",
       "Date                                                                           \n",
       "2021-01-08  214.771872  214.948429   45.644519      0.007148  0.438209      0  \n",
       "2021-01-11  213.830935  214.640966  -61.141120      0.006850  0.126989      1  \n",
       "2021-01-12  211.345468  213.719760 -138.656181      0.011183 -2.315138      1  \n",
       "2021-01-13  210.347737  213.271824  -53.236638      0.008278 -2.831279      0  \n",
       "2021-01-14  206.103869  211.389589  -86.102763      0.020698 -5.181079      1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([649, 3, 30]),\n",
       " torch.Size([649]),\n",
       " torch.Size([70, 3, 30]),\n",
       " torch.Size([70]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split and preprocess the data separately to avoid data leakage\n",
    "def create_dataset_v2(dataset, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset)-window_size):\n",
    "        features, target = dataset[i:i+window_size, :-1], dataset[i+window_size, -1]\n",
    "        X.append(features)\n",
    "        y.append(target)\n",
    "    return torch.tensor(np.array(X), dtype=torch.float), torch.tensor(np.array(y), dtype=torch.long)\n",
    "\n",
    "X = full_df.to_numpy()\n",
    "\n",
    "TRAIN_PROPORTION = 0.9\n",
    "train_size = int(TRAIN_PROPORTION * X.shape[0])\n",
    "\n",
    "train_features, train_labels = X[:train_size, :-1], X[:train_size, -1].reshape(-1,1)\n",
    "test_features, test_labels = X[train_size:, :-1], X[train_size:, -1].reshape(-1,1)\n",
    "\n",
    "train_scaler = RobustScaler() #  MinMaxScaler(feature_range=(-1,1))\n",
    "train_features_scaled = train_scaler.fit_transform(train_features)\n",
    "one_hot_train_labels = np.eye(3)[train_labels.astype(int).reshape(-1)] # Add one-hot encoding of labels from previous days as features\n",
    "train_dataset = np.concatenate([train_features_scaled, one_hot_train_labels, train_labels], axis=1)\n",
    "\n",
    "test_scaler = RobustScaler() # MinMaxScaler(feature_range=(-1,1))\n",
    "test_features_scaled = test_scaler.fit_transform(test_features)\n",
    "one_hot_test_labels = np.eye(3)[test_labels.astype(int).reshape(-1)] # Add one-hot encoding of labels from previous days as features\n",
    "test_dataset = np.concatenate([test_features_scaled, one_hot_test_labels, test_labels], axis=1)\n",
    "\n",
    "X_train, y_train = create_dataset_v2(train_dataset, window_size=WINDOW_SIZE)\n",
    "X_test, y_test = create_dataset_v2(test_dataset, window_size=WINDOW_SIZE)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = self.input_dim, \n",
    "                            hidden_size = self.hidden_dim, \n",
    "                            num_layers=self.num_layers, \n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=self.hidden_dim, out_features=output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).requires_grad_().to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        _, (hn, _) = self.lstm(x, (h0, c0)) \n",
    "        out = self.linear(hn[-1]) # Last hidden state\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIM = len(tickers)\n",
    "\n",
    "def validate_model(batch_size, hidden_dim, learning_rate, num_epochs, display_progress=False):\n",
    "    model = LSTM(input_dim=X_train.shape[2], hidden_dim=hidden_dim, num_layers=1, output_dim=OUTPUT_DIM).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    loader = DataLoader(TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(num_epochs+1):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred_logits = model(X_batch)\n",
    "            loss = loss_fn(y_pred_logits, y_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if display_progress:\n",
    "            if epoch % 20 != 0:\n",
    "                continue\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred_logits = model(X_train)\n",
    "                train_cross_entropy_loss = loss_fn(y_pred_logits, y_train) # CrossEntropyLoss expects logits, not probabilities\n",
    "                print(\"Epoch %d: train Cross Entropy Loss %.4f\" % (epoch, train_cross_entropy_loss.item()))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_logits = model(X_test)\n",
    "        test_cross_entropy_loss = loss_fn(y_pred_logits, y_test).item()\n",
    "        y_pred_labels = torch.argmax(y_pred_logits, axis=1) # not necessary to use softmax for accuracy calculation, since max of softmax is also max of probabilities\n",
    "        accuracy = accuracy_score(y_test.cpu().numpy(), y_pred_labels.cpu().numpy())\n",
    "        return accuracy, test_cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train Cross Entropy Loss 1.0611\n",
      "Epoch 20: train Cross Entropy Loss 0.0106\n",
      "Epoch 40: train Cross Entropy Loss 0.0016\n",
      "Epoch 60: train Cross Entropy Loss 0.0007\n",
      "Epoch 80: train Cross Entropy Loss 0.0004\n",
      "Epoch 100: train Cross Entropy Loss 0.0003\n",
      "Epoch 0: train Cross Entropy Loss 1.0636\n",
      "Epoch 20: train Cross Entropy Loss 0.3008\n",
      "Epoch 40: train Cross Entropy Loss 0.0092\n",
      "Epoch 60: train Cross Entropy Loss 0.0028\n",
      "Epoch 80: train Cross Entropy Loss 0.0015\n",
      "Epoch 100: train Cross Entropy Loss 0.0009\n",
      "Best accuracy: 0.3857\n",
      "Best hyperparameters: (64, 64, 0.01, 100)\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [64,128] # [16, 32, 64, 128]\n",
    "hidden_size_values = [64] # [32, 64, 128]\n",
    "learning_rate_values = [0.01] #[0.1, 0.01, 0.001, 0.0001]\n",
    "num_epochs_values = [100] # [50, 100, 300, 1000]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_hyperparameters = None\n",
    "i = 0\n",
    "\n",
    "for (batch_size, hidden_dim, learning_rate, num_epochs) in product(batch_sizes, hidden_size_values, learning_rate_values, num_epochs_values):\n",
    "    curr_accuracy, curr_loss = validate_model(batch_size, hidden_dim, learning_rate, num_epochs, display_progress=True)\n",
    "    if curr_accuracy > best_accuracy:\n",
    "        best_accuracy = curr_accuracy\n",
    "        best_hyperparameters = (batch_size, hidden_dim, learning_rate, num_epochs)\n",
    "    i += 1\n",
    "    if i % 5 == 0:\n",
    "        print(f\"{i} models trained\")\n",
    "\n",
    "print(\"Best accuracy: %.4f\" % best_accuracy)\n",
    "print(\"Best hyperparameters: %s\" % str(best_hyperparameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train Cross Entropy Loss 1.0614\n",
      "Epoch 20: train Cross Entropy Loss 0.0100\n",
      "Epoch 40: train Cross Entropy Loss 0.0016\n",
      "Epoch 60: train Cross Entropy Loss 0.0007\n",
      "Epoch 80: train Cross Entropy Loss 0.0004\n",
      "Epoch 100: train Cross Entropy Loss 0.0003\n",
      "Testing accuracy 0.4286\n",
      "[[ 7 11 10]\n",
      " [ 5 11  7]\n",
      " [ 6  1 12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.25      0.30        28\n",
      "           1       0.48      0.48      0.48        23\n",
      "           2       0.41      0.63      0.50        19\n",
      "\n",
      "    accuracy                           0.43        70\n",
      "   macro avg       0.43      0.45      0.43        70\n",
      "weighted avg       0.43      0.43      0.41        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size, hidden_dim, learning_rate, num_epochs = best_hyperparameters\n",
    "model = LSTM(input_dim=X_train.shape[2], hidden_dim=hidden_dim, num_layers=1, output_dim=OUTPUT_DIM).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "loader = DataLoader(TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred_logits = model(X_batch)\n",
    "        loss = loss_fn(y_pred_logits, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch % 20 != 0:\n",
    "        continue\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_logits = model(X_train)\n",
    "        train_cross_entropy_loss = loss_fn(y_pred_logits, y_train)\n",
    "        print(\"Epoch %d: train Cross Entropy Loss %.4f\" % (epoch, train_cross_entropy_loss.item()))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_logits = model(X_test)\n",
    "    y_pred_labels = torch.argmax(y_pred_logits, axis=1) # not necessary to use softmax for accuracy calculation, since max of softmax is also max of probabilities\n",
    "    print(\"Testing accuracy %.4f\" % accuracy_score(y_test.cpu().numpy(), y_pred_labels.cpu().numpy()))\n",
    "    print(confusion_matrix(y_test.cpu().numpy(), y_pred_labels.cpu().numpy()))\n",
    "    print(classification_report(y_test.cpu().numpy(), y_pred_labels.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random accuracy: 0.3350\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "for _ in range(10000):\n",
    "    rand_preds = torch.tensor(np.random.randint(0, len(tickers), size=y_test.shape[0])).to(device)\n",
    "    # print(\"Testing accuracy %.4f\" % accuracy_score(y_test.cpu().numpy(), rand_preds.cpu().numpy()))\n",
    "    # print(confusion_matrix(y_test.cpu().numpy(), rand_preds.cpu().numpy()))\n",
    "    # print(classification_report(y_test.cpu().numpy(), rand_preds.cpu().numpy()))\n",
    "    accuracies.append(accuracy_score(y_test.cpu().numpy(), rand_preds.cpu().numpy()))\n",
    "\n",
    "print(\"Random accuracy: %.4f\" % np.mean(accuracies))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
